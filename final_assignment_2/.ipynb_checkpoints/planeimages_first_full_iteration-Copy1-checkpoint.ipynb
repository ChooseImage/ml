{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "from skimage import io, color, transform, feature, filters\n",
    "from my_measures import BinaryClassificationPerformance  \n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from skimage.feature import corner_foerstner, corner_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT!!! Make sure you are using BinaryClassificationPerformance v1.03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(BinaryClassificationPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file paths and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_path = 'plane_data/cropped_images/' # file path for cropped images for training\n",
    "l_file = 'plane_data/plane_labels.csv' # file path and file name for csv with labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for feature building and extraction on photographsÂ¶\n",
    "\n",
    "scikit-image documentation on methods used for feature extraction:  \n",
    "\n",
    "* http://scikit-image.org/docs/dev/api/skimage.color.html#rgb2gray  \n",
    "* http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize  \n",
    "* http://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.canny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in downscaling the image, what do you want the new dimensions to be?\n",
    "# the original dimensions of cropped images: (60, 140), which if 8,400 pixels\n",
    "dims = (15, 35) # 25% of the original size, 525 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downscaled image shape: \n",
      "(15, 35)\n",
      "image representation (first row of pixels): \n",
      "[ 2.21964971e-03  2.37261530e-03  4.26815983e-04  8.18626382e-05\n",
      "  1.29192535e-04  1.17677613e-06 -1.31530707e-04 -2.74339080e-04\n",
      " -3.68094965e-04 -4.35450149e-04 -5.30501135e-04 -7.04805977e-04\n",
      " -9.99332915e-04 -1.35147273e-03 -1.47396934e-03 -1.48908188e-03\n",
      " -1.45129621e-03 -1.06631147e-03 -7.71478537e-04 -5.04495764e-04\n",
      "  7.07032445e-05  3.61778707e-04  2.06908189e-04 -9.44793259e-06\n",
      " -1.34745775e-04 -8.23778532e-05  1.88157762e-04  8.09897873e-04\n",
      "  2.75437846e-03  7.43559693e-03  7.46968679e-03  2.99938478e-03\n",
      "  2.22650653e-03  3.45763225e-03  2.76462677e-03]\n",
      "\n",
      "\n",
      "example of transformation: \n"
     ]
    }
   ],
   "source": [
    "def image_manipulation(imname, imgs_path, imview=False):\n",
    "    warnings.filterwarnings('ignore')\n",
    "    imname = imgs_path + imname + '.png'\n",
    "    img_raw = io.imread(imname, as_gray=True)\n",
    "    downscaled = transform.resize(img_raw, (dims[0], dims[1])) # downscale image\n",
    "    \n",
    "    final_image = feature.canny(downscaled, sigma=1) # edge filter image with Canny algorithm\n",
    "    final_image = feature.corner_harris(downscaled) \n",
    "    \n",
    "    if imview==True:\n",
    "        plt.figure()\n",
    "        plt.imshow(final_image, cmap='Greys')\n",
    "        plt.show()\n",
    "    warnings.filterwarnings('always')\n",
    "    return final_image\n",
    "\n",
    "# test the function, look at input/output\n",
    "test_image = image_manipulation('2017-08-25T23+24+13_390Z', ci_path, True)\n",
    "print('downscaled image shape: ')\n",
    "print(test_image.shape)\n",
    "print('image representation (first row of pixels): ')\n",
    "print(test_image[0])\n",
    "print('\\n')\n",
    "print('example of transformation: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for comparison, look at original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#this_imname = ci_path + '2017-08-25T23+24+13_390Z.png'\n",
    "#io.imshow(io.imread(this_imname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to process raw images, resulting in training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes raw images and completes all preprocessing required before model fits\n",
    "def process_raw_data(labels_fn, images_fp, my_random_seed, imview=False, test=False):\n",
    "    plane_data = pd.read_csv(labels_fn) # read in photograph labels\n",
    "    print(\"First few lines of image labels: \")\n",
    "    print(plane_data.head())\n",
    "    print(\"Size of image label dataFrame: \")\n",
    "    print(plane_data.shape)\n",
    "        \n",
    "    # construct lists for features, labels, and a crosswalk reference to image names\n",
    "    features_list = []\n",
    "    if (not test):\n",
    "        y_list = []\n",
    "    imnames_list = []\n",
    "\n",
    "    for index, row in plane_data.iterrows():\n",
    "        features_list.append(image_manipulation(row['img_name'], images_fp))\n",
    "        if (not test):\n",
    "            y_list.append(row['plane'])\n",
    "        imnames_list.append(row['img_name'])\n",
    "    \n",
    "    # convert the lists to ndarrays\n",
    "    features = np.asarray(features_list)\n",
    "    if (not test):\n",
    "        Y = np.asarray(y_list)\n",
    "    imgs = np.asarray(imnames_list)\n",
    "    print('Shape of original feature representation: ')\n",
    "    print(features.shape)\n",
    "\n",
    "    # flatten the images ndarray to one row per image\n",
    "    features_flat = features.reshape((features.shape[0], -1))\n",
    "\n",
    "    print('Shape of flat feature representation: ')\n",
    "    print(features_flat.shape)\n",
    "\n",
    "    if (not test):\n",
    "        print('Shape of Y: ')\n",
    "        print(Y.shape)\n",
    "\n",
    "        print('Number of images with planes: ')\n",
    "        print(Y.sum())\n",
    "    \n",
    "        # create train and test sets\n",
    "        data_train, data_test, y_train, y_test, imgs_train, imgs_test = train_test_split(features_flat, \n",
    "            Y, imgs, test_size = 0.25, random_state = my_random_seed)\n",
    "\n",
    "        print('Shape of training set: ')\n",
    "        print(y_train.shape)\n",
    "        print('Number of training images that contain an airplane: ')\n",
    "        print(y_train.sum())\n",
    "\n",
    "        print('Shape of test set: ')\n",
    "        print(y_test.shape)\n",
    "        print('Number of test images that contain an airplane: ')\n",
    "        print(y_test.sum())\n",
    "    \n",
    "    if (test):\n",
    "        X_submission_test = features_flat\n",
    "        print(\"Shape of X_test for submission:\")\n",
    "        print(X_submission_test.shape)\n",
    "        print('SUCCESS!')\n",
    "        return(X_submission_test, plane_data)\n",
    "    else: \n",
    "        print(\"Shape of data_train and data_test:\")\n",
    "        print(data_train.shape)\n",
    "        print(data_test.shape)\n",
    "        print(\"Shape of y_train and y_test:\")\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        print(\"Shape of imgs_train and imgs_test:\")\n",
    "        print(imgs_train.shape)\n",
    "        print(imgs_test.shape)\n",
    "        print('SUCCESS!')\n",
    "        return(data_train, data_test, y_train, y_test, imgs_train, imgs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of image labels: \n",
      "                   img_name  plane\n",
      "0  2016-08-02T13+50+24_430Z  False\n",
      "1  2016-08-02T14+12+37_390Z  False\n",
      "2  2016-08-02T22+20+26_600Z  False\n",
      "3  2016-08-03T12+04+30_670Z  False\n",
      "4  2016-08-03T12+32+21_790Z  False\n",
      "Size of image label dataFrame: \n",
      "(6758, 2)\n",
      "Shape of original feature representation: \n",
      "(6758, 15, 35)\n",
      "Shape of flat feature representation: \n",
      "(6758, 525)\n",
      "Shape of Y: \n",
      "(6758,)\n",
      "Number of images with planes: \n",
      "101\n",
      "Shape of training set: \n",
      "(5068,)\n",
      "Number of training images that contain an airplane: \n",
      "76\n",
      "Shape of test set: \n",
      "(1690,)\n",
      "Number of test images that contain an airplane: \n",
      "25\n",
      "Shape of data_train and data_test:\n",
      "(5068, 525)\n",
      "(1690, 525)\n",
      "Shape of y_train and y_test:\n",
      "(5068,)\n",
      "(1690,)\n",
      "Shape of imgs_train and imgs_test:\n",
      "(5068,)\n",
      "(1690,)\n",
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test, y_train, y_test, imgs_train, imgs_test = process_raw_data(l_file, ci_path, \n",
    "    my_random_seed=17, imview=False, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET: \n",
      "{'Pos': 76, 'Neg': 4992, 'TP': 56, 'TN': 4982, 'FP': 10, 'FN': 20, 'Accuracy': 0.9940805051302289, 'Precision': 0.8484848484848485, 'Recall': 0.7368421052631579, 'desc': 'prc', 'set': 'train'}\n",
      "TEST SET: \n",
      "{'Pos': 25, 'Neg': 1665, 'TP': 9, 'TN': 1654, 'FP': 11, 'FN': 16, 'Accuracy': 0.9840236686390532, 'Precision': 0.45, 'Recall': 0.36, 'desc': 'prc', 'set': 'test'}\n"
     ]
    }
   ],
   "source": [
    "# MODEL: Perceptron\n",
    "from sklearn import linear_model\n",
    "prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "prc.fit(data_train, y_train)\n",
    "\n",
    "prc_performance = BinaryClassificationPerformance(prc.predict(data_train), y_train, 'prc')\n",
    "prc_performance.compute_measures()\n",
    "prc_performance.performance_measures['set'] = 'train'\n",
    "print('TRAINING SET: ')\n",
    "print(prc_performance.performance_measures)\n",
    "\n",
    "prc_performance_test = BinaryClassificationPerformance(prc.predict(data_test), y_test, 'prc')\n",
    "prc_performance_test.compute_measures()\n",
    "prc_performance_test.performance_measures['set'] = 'test'\n",
    "print('TEST SET: ')\n",
    "print(prc_performance_test.performance_measures)\n",
    "\n",
    "prc_performance_test.img_indices()\n",
    "prc_img_indices_to_view = prc_performance_test.image_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_examples(typ, measures):\n",
    "    iiv = ''\n",
    "    if typ == 'FP':\n",
    "        iiv = typ + '_indices'\n",
    "    elif typ == 'TP':\n",
    "        iiv = typ + '_indices'\n",
    "    elif typ == 'FN':\n",
    "        iiv = typ + '_indices'\n",
    "    else:\n",
    "        raise ValueError('input must be \"TP\", \"FP\", or \"FN\"')\n",
    "    for img in measures[iiv]:\n",
    "        warnings.filterwarnings('ignore')    \n",
    "        plt.figure()\n",
    "        lookat = ci_path + imgs_test[img] + '.png' # location of original image\n",
    "        io.imshow(lookat) # show original image\n",
    "        plt.figure()\n",
    "        plt.imshow(data_test[img].reshape(dims[0], dims[1])) # show manipulation for feature representation\n",
    "        warnings.filterwarnings('always')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at examples of Perceptron classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_examples('TP', prc_img_indices_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_examples('FP', prc_img_indices_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_examples('FN', prc_img_indices_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train Multilayer Perceptron, a.k.a. neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(max_iter=1000)\n"
     ]
    }
   ],
   "source": [
    "# MODEL: Multi-layer Perceptron aka neural network\n",
    "from sklearn import neural_network\n",
    "nn = neural_network.MLPClassifier(max_iter=1000)\n",
    "print(nn)\n",
    "nn.fit(data_train, y_train)\n",
    "\n",
    "nn_performance = BinaryClassificationPerformance(nn.predict(data_train), y_train, 'nn')\n",
    "nn_performance.compute_measures()\n",
    "nn_performance.performance_measures['set'] = 'train'\n",
    "print('TRAINING SET: ')\n",
    "print(nn_performance.performance_measures)\n",
    "\n",
    "nn_performance_test = BinaryClassificationPerformance(nn.predict(data_test), y_test, 'nn_test')\n",
    "nn_performance_test.compute_measures()\n",
    "nn_performance_test.performance_measures['set'] = 'test'\n",
    "print('TEST SET: ')\n",
    "print(nn_performance_test.performance_measures)\n",
    "\n",
    "nn_performance_test.img_indices()\n",
    "nn_img_indices_to_view = nn_performance_test.image_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look at examples of neural network classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## true positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_examples('TP', nn_img_indices_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_examples('FP', nn_img_indices_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_examples('FN', nn_img_indices_to_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of fits to compare: \n",
    "final_fits = []\n",
    "final_fits.append(prc_performance.performance_measures)\n",
    "final_fits.append(prc_performance_test.performance_measures)\n",
    "final_fits.append(nn_performance.performance_measures)\n",
    "final_fits.append(nn_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for fit in final_fits:\n",
    "    if fit['set'] == 'train':\n",
    "        color = 'co'\n",
    "    else:\n",
    "        color = 'ro'\n",
    "    plt.plot(fit['FP'] / fit['Neg'], \n",
    "             fit['TP'] / fit['Pos'], color, markersize=12)\n",
    "    plt.text(fit['FP'] / fit['Neg'], \n",
    "             fit['TP'] / fit['Pos'], fit['desc'] + ': ' + fit['set'], fontsize=16)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of fits to compare: \n",
    "final_fits = []\n",
    "final_fits.append(prc_performance.performance_measures)\n",
    "final_fits.append(prc_performance_test.performance_measures)\n",
    "final_fits.append(nn_performance.performance_measures)\n",
    "final_fits.append(nn_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for fit in final_fits:\n",
    "    if fit['set'] == 'train':\n",
    "        color = 'co'\n",
    "    else:\n",
    "        color = 'ro'\n",
    "    plt.plot(fit['FP'] / fit['Neg'], \n",
    "             fit['TP'] / fit['Pos'], color, markersize=12)\n",
    "    plt.text(fit['FP'] / fit['Neg'], \n",
    "             fit['TP'] / fit['Pos'], fit['desc'] + ': ' + fit['set'], fontsize=16)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION\n",
    "\n",
    "### file paths and names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ci_path = 'test_data_for_grading/test_cropped_images/' # file path for cropped images for training\n",
    "submission_l_file = 'test_data_for_grading/test_plane_labels.csv' # file path and file name for csv with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines of image labels: \n",
      "                   img_name\n",
      "0  2016-08-02T13+50+24_430Z\n",
      "1  2016-08-02T14+12+37_390Z\n",
      "2  2016-08-03T12+32+21_790Z\n",
      "3  2016-08-03T13+19+28_320Z\n",
      "4  2016-08-05T15+24+58_670Z\n",
      "Size of image label dataFrame: \n",
      "(1523, 1)\n",
      "Shape of original feature representation: \n",
      "(1523, 15, 35)\n",
      "Shape of flat feature representation: \n",
      "(1523, 525)\n",
      "Shape of X_test for submission:\n",
      "(1523, 525)\n",
      "SUCCESS!\n",
      "Number of rows in the submission test set (should be 1,523): \n"
     ]
    }
   ],
   "source": [
    "X_test_data, X_test_submission = process_raw_data(submission_l_file, submission_ci_path, my_random_seed=74, test=True)\n",
    "print(\"Number of rows in the submission test set (should be 1,523): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT CHECK: make sure that the number of columns in your training data is the same as the number of columns in this test submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068, 525)\n",
      "(1523, 525)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(X_test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training set and submission test set have 525 columns. Success!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Choose a *single* model for your submission. In this code, I am choosing the Perceptron model fit, which is in the prc object. But you should choose the model that is performing the best for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016414970453053186\n"
     ]
    }
   ],
   "source": [
    "# concatenate predictions to the id\n",
    "X_test_submission[\"prediction\"] = prc.predict(X_test_data)\n",
    "# look at the proportion of positive predictions\n",
    "print(X_test_submission['prediction'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the proportion of predictions that have predicted that there is an airplane in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1523, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_submission.shape) # should be (1523, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export submission file as pdf\n",
    "# CHANGE FILE PATH: \n",
    "X_test_submission.to_csv('airplane_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
